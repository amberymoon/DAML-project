{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19445254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from modules.training import train_model\n",
    "\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import resnet18\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e69c31",
   "metadata": {},
   "source": [
    "# Import data post-CNN and train a shallow network -> Best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c2d8048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: torch.Size([613, 512]), torch.Size([613])\n",
      "Validation data shape: torch.Size([216, 512]), torch.Size([216])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                    [-1, 4]           2,052\n",
      "================================================================\n",
      "Total params: 2,052\n",
      "Trainable params: 2,052\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.01\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load the training data\n",
    "data_train = torch.load('data_images/train.pt')\n",
    "X_train = data_train['Features'][:613, :]  # shape: (N, H)\n",
    "Y_train = data_train['Targets'][:613]  # shape: (N,)\n",
    "\n",
    "# Load the validation data\n",
    "data_val = torch.load('data_images/valid.pt')\n",
    "X_val = data_val['Features']  # shape: (N, H)\n",
    "Y_val = data_val['Targets']  # shape: (N,)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}, {Y_train.shape}\")\n",
    "print(f\"Validation data shape: {X_val.shape}, {Y_val.shape}\")\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_ds = TensorDataset(X_train, Y_train)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_ds = TensorDataset(X_train, Y_train)\n",
    "val_loader = DataLoader(train_ds, shuffle=True)\n",
    "\n",
    "shared_weights = nn.Linear(512, 4)\n",
    "\n",
    "\n",
    "# Define a simple NN\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, X, Y):\n",
    "        super().__init__()\n",
    "        self.fc1 = deepcopy(shared_weights)\n",
    "\n",
    "        # Loss and optimizer are defined here:\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        # x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNN(X_train, Y_train)\n",
    "print(summary(model, input_size=X_train.shape[1:]))\n",
    "\n",
    "# train_model(model, train_loader, val_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1815df49",
   "metadata": {},
   "source": [
    "# Import images, preprocess them using resnet with fc=Identity() then train the same simple network -> Best results\n",
    "Results should be the same as before and they are: at last epoch (100) we get Train Acc: 0.961 | Val Acc: 0.964"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b15a818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/DAML-project/data_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/DAML-project/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/DAML-project/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "Epoch 1/100 | Train Loss: 26.492 | Train Acc: 0.380 | Val Acc: 0.542\n",
      "Epoch 2/100 | Train Loss: 19.768 | Train Acc: 0.582 | Val Acc: 0.582\n",
      "Epoch 3/100 | Train Loss: 17.692 | Train Acc: 0.659 | Val Acc: 0.708\n",
      "Epoch 4/100 | Train Loss: 16.102 | Train Acc: 0.674 | Val Acc: 0.750\n",
      "Epoch 5/100 | Train Loss: 15.416 | Train Acc: 0.716 | Val Acc: 0.744\n",
      "Epoch 6/100 | Train Loss: 14.192 | Train Acc: 0.741 | Val Acc: 0.757\n",
      "Epoch 7/100 | Train Loss: 13.918 | Train Acc: 0.718 | Val Acc: 0.736\n",
      "Epoch 8/100 | Train Loss: 13.120 | Train Acc: 0.723 | Val Acc: 0.768\n",
      "Epoch 9/100 | Train Loss: 12.383 | Train Acc: 0.767 | Val Acc: 0.768\n",
      "Epoch 10/100 | Train Loss: 11.882 | Train Acc: 0.790 | Val Acc: 0.780\n",
      "Epoch 11/100 | Train Loss: 11.354 | Train Acc: 0.801 | Val Acc: 0.837\n",
      "Epoch 12/100 | Train Loss: 11.730 | Train Acc: 0.814 | Val Acc: 0.835\n",
      "Epoch 13/100 | Train Loss: 10.715 | Train Acc: 0.822 | Val Acc: 0.811\n",
      "Epoch 14/100 | Train Loss: 10.503 | Train Acc: 0.829 | Val Acc: 0.801\n",
      "Epoch 15/100 | Train Loss: 10.157 | Train Acc: 0.850 | Val Acc: 0.856\n",
      "Epoch 16/100 | Train Loss: 9.829 | Train Acc: 0.835 | Val Acc: 0.865\n",
      "Epoch 17/100 | Train Loss: 9.736 | Train Acc: 0.861 | Val Acc: 0.873\n",
      "Epoch 18/100 | Train Loss: 9.392 | Train Acc: 0.866 | Val Acc: 0.853\n",
      "Epoch 19/100 | Train Loss: 9.137 | Train Acc: 0.852 | Val Acc: 0.874\n",
      "Epoch 20/100 | Train Loss: 8.866 | Train Acc: 0.879 | Val Acc: 0.868\n",
      "Epoch 21/100 | Train Loss: 8.895 | Train Acc: 0.860 | Val Acc: 0.892\n",
      "Epoch 22/100 | Train Loss: 8.608 | Train Acc: 0.886 | Val Acc: 0.858\n",
      "Epoch 23/100 | Train Loss: 8.299 | Train Acc: 0.889 | Val Acc: 0.856\n",
      "Epoch 24/100 | Train Loss: 8.205 | Train Acc: 0.861 | Val Acc: 0.894\n",
      "Epoch 25/100 | Train Loss: 8.108 | Train Acc: 0.887 | Val Acc: 0.891\n",
      "Epoch 26/100 | Train Loss: 8.216 | Train Acc: 0.887 | Val Acc: 0.909\n",
      "Epoch 27/100 | Train Loss: 7.798 | Train Acc: 0.896 | Val Acc: 0.912\n",
      "Epoch 28/100 | Train Loss: 7.771 | Train Acc: 0.902 | Val Acc: 0.912\n",
      "Epoch 29/100 | Train Loss: 7.449 | Train Acc: 0.896 | Val Acc: 0.909\n",
      "Epoch 30/100 | Train Loss: 7.706 | Train Acc: 0.886 | Val Acc: 0.892\n",
      "Epoch 31/100 | Train Loss: 7.301 | Train Acc: 0.910 | Val Acc: 0.915\n",
      "Epoch 32/100 | Train Loss: 7.357 | Train Acc: 0.902 | Val Acc: 0.904\n",
      "Epoch 33/100 | Train Loss: 7.123 | Train Acc: 0.909 | Val Acc: 0.910\n",
      "Epoch 34/100 | Train Loss: 7.291 | Train Acc: 0.912 | Val Acc: 0.920\n",
      "Epoch 35/100 | Train Loss: 6.800 | Train Acc: 0.909 | Val Acc: 0.920\n",
      "Epoch 36/100 | Train Loss: 6.873 | Train Acc: 0.915 | Val Acc: 0.915\n",
      "Epoch 37/100 | Train Loss: 6.857 | Train Acc: 0.909 | Val Acc: 0.922\n",
      "Epoch 38/100 | Train Loss: 6.390 | Train Acc: 0.912 | Val Acc: 0.917\n",
      "Epoch 39/100 | Train Loss: 6.480 | Train Acc: 0.920 | Val Acc: 0.917\n",
      "Epoch 40/100 | Train Loss: 6.517 | Train Acc: 0.910 | Val Acc: 0.925\n",
      "Epoch 41/100 | Train Loss: 6.670 | Train Acc: 0.912 | Val Acc: 0.925\n",
      "Epoch 42/100 | Train Loss: 6.476 | Train Acc: 0.902 | Val Acc: 0.927\n",
      "Epoch 43/100 | Train Loss: 6.442 | Train Acc: 0.925 | Val Acc: 0.922\n",
      "Epoch 44/100 | Train Loss: 6.075 | Train Acc: 0.922 | Val Acc: 0.928\n",
      "Epoch 45/100 | Train Loss: 6.141 | Train Acc: 0.928 | Val Acc: 0.928\n",
      "Epoch 46/100 | Train Loss: 6.100 | Train Acc: 0.930 | Val Acc: 0.928\n",
      "Epoch 47/100 | Train Loss: 6.132 | Train Acc: 0.920 | Val Acc: 0.931\n",
      "Epoch 48/100 | Train Loss: 5.879 | Train Acc: 0.930 | Val Acc: 0.935\n",
      "Epoch 49/100 | Train Loss: 5.987 | Train Acc: 0.936 | Val Acc: 0.925\n",
      "Epoch 50/100 | Train Loss: 6.080 | Train Acc: 0.925 | Val Acc: 0.935\n",
      "Epoch 51/100 | Train Loss: 5.569 | Train Acc: 0.931 | Val Acc: 0.935\n",
      "Epoch 52/100 | Train Loss: 5.612 | Train Acc: 0.927 | Val Acc: 0.931\n",
      "Epoch 53/100 | Train Loss: 5.827 | Train Acc: 0.936 | Val Acc: 0.936\n",
      "Epoch 54/100 | Train Loss: 5.561 | Train Acc: 0.933 | Val Acc: 0.931\n",
      "Epoch 55/100 | Train Loss: 5.838 | Train Acc: 0.933 | Val Acc: 0.940\n",
      "Epoch 56/100 | Train Loss: 5.579 | Train Acc: 0.933 | Val Acc: 0.938\n",
      "Epoch 57/100 | Train Loss: 5.387 | Train Acc: 0.935 | Val Acc: 0.940\n",
      "Epoch 58/100 | Train Loss: 5.495 | Train Acc: 0.931 | Val Acc: 0.938\n",
      "Epoch 59/100 | Train Loss: 5.261 | Train Acc: 0.936 | Val Acc: 0.943\n",
      "Epoch 60/100 | Train Loss: 5.194 | Train Acc: 0.938 | Val Acc: 0.933\n",
      "Epoch 61/100 | Train Loss: 5.275 | Train Acc: 0.936 | Val Acc: 0.941\n",
      "Epoch 62/100 | Train Loss: 5.202 | Train Acc: 0.948 | Val Acc: 0.938\n",
      "Epoch 63/100 | Train Loss: 5.396 | Train Acc: 0.941 | Val Acc: 0.943\n",
      "Epoch 64/100 | Train Loss: 5.078 | Train Acc: 0.943 | Val Acc: 0.948\n",
      "Epoch 65/100 | Train Loss: 5.121 | Train Acc: 0.949 | Val Acc: 0.951\n",
      "Epoch 66/100 | Train Loss: 5.094 | Train Acc: 0.943 | Val Acc: 0.948\n",
      "Epoch 67/100 | Train Loss: 5.142 | Train Acc: 0.956 | Val Acc: 0.951\n",
      "Epoch 68/100 | Train Loss: 5.029 | Train Acc: 0.948 | Val Acc: 0.948\n",
      "Epoch 69/100 | Train Loss: 5.084 | Train Acc: 0.953 | Val Acc: 0.953\n",
      "Epoch 70/100 | Train Loss: 4.881 | Train Acc: 0.949 | Val Acc: 0.949\n",
      "Epoch 71/100 | Train Loss: 4.788 | Train Acc: 0.949 | Val Acc: 0.954\n",
      "Epoch 72/100 | Train Loss: 5.144 | Train Acc: 0.956 | Val Acc: 0.948\n",
      "Epoch 73/100 | Train Loss: 5.073 | Train Acc: 0.946 | Val Acc: 0.953\n",
      "Epoch 74/100 | Train Loss: 4.725 | Train Acc: 0.951 | Val Acc: 0.951\n",
      "Epoch 75/100 | Train Loss: 4.667 | Train Acc: 0.951 | Val Acc: 0.956\n",
      "Epoch 76/100 | Train Loss: 4.613 | Train Acc: 0.959 | Val Acc: 0.945\n",
      "Epoch 77/100 | Train Loss: 4.667 | Train Acc: 0.938 | Val Acc: 0.953\n",
      "Epoch 78/100 | Train Loss: 4.616 | Train Acc: 0.953 | Val Acc: 0.958\n",
      "Epoch 79/100 | Train Loss: 4.590 | Train Acc: 0.956 | Val Acc: 0.956\n",
      "Epoch 80/100 | Train Loss: 4.625 | Train Acc: 0.954 | Val Acc: 0.958\n",
      "Epoch 81/100 | Train Loss: 4.560 | Train Acc: 0.958 | Val Acc: 0.956\n",
      "Epoch 82/100 | Train Loss: 4.544 | Train Acc: 0.959 | Val Acc: 0.959\n",
      "Epoch 83/100 | Train Loss: 4.589 | Train Acc: 0.953 | Val Acc: 0.962\n",
      "Epoch 84/100 | Train Loss: 4.439 | Train Acc: 0.961 | Val Acc: 0.958\n",
      "Epoch 85/100 | Train Loss: 4.528 | Train Acc: 0.956 | Val Acc: 0.951\n",
      "Epoch 86/100 | Train Loss: 4.367 | Train Acc: 0.961 | Val Acc: 0.961\n",
      "Epoch 87/100 | Train Loss: 4.325 | Train Acc: 0.961 | Val Acc: 0.961\n",
      "Epoch 88/100 | Train Loss: 4.524 | Train Acc: 0.959 | Val Acc: 0.961\n",
      "Epoch 89/100 | Train Loss: 4.428 | Train Acc: 0.964 | Val Acc: 0.956\n",
      "Epoch 90/100 | Train Loss: 4.364 | Train Acc: 0.961 | Val Acc: 0.962\n",
      "Epoch 91/100 | Train Loss: 4.296 | Train Acc: 0.961 | Val Acc: 0.962\n",
      "Epoch 92/100 | Train Loss: 4.233 | Train Acc: 0.962 | Val Acc: 0.964\n",
      "Epoch 93/100 | Train Loss: 4.258 | Train Acc: 0.961 | Val Acc: 0.966\n",
      "Epoch 94/100 | Train Loss: 4.094 | Train Acc: 0.964 | Val Acc: 0.962\n",
      "Epoch 95/100 | Train Loss: 4.138 | Train Acc: 0.962 | Val Acc: 0.964\n",
      "Epoch 96/100 | Train Loss: 4.519 | Train Acc: 0.958 | Val Acc: 0.962\n",
      "Epoch 97/100 | Train Loss: 4.144 | Train Acc: 0.962 | Val Acc: 0.962\n",
      "Epoch 98/100 | Train Loss: 4.218 | Train Acc: 0.964 | Val Acc: 0.964\n",
      "Epoch 99/100 | Train Loss: 4.074 | Train Acc: 0.964 | Val Acc: 0.964\n",
      "Epoch 100/100 | Train Loss: 4.175 | Train Acc: 0.961 | Val Acc: 0.966\n"
     ]
    }
   ],
   "source": [
    "dataset_path = os.path.join(os.getcwd(), 'data_images')\n",
    "print(dataset_path)\n",
    "\n",
    "# Mean e std per immagini RGB normalizzate su [-1, 1]\n",
    "mean = [0.5, 0.5, 0.5]\n",
    "std = [0.5, 0.5, 0.5]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, \"train\"), transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, \"valid\"), transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "\n",
    "model = resnet18(pretrained=True)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.fc = nn.Identity()\n",
    "\n",
    "if True:  # Change to True if you want to train the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in train_loader:\n",
    "            print(\"starting\")\n",
    "            outputs = model(inputs)\n",
    "            mapped_data_train = outputs\n",
    "            labels_train = labels\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            mapped_data_val = outputs\n",
    "            labels_val = labels\n",
    "\n",
    "    train_ds = TensorDataset(mapped_data_train, labels_train)\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "    val_ds = TensorDataset(mapped_data_val, labels_val)\n",
    "    val_loader = DataLoader(train_ds, shuffle=False)\n",
    "\n",
    "    model = SimpleNN(mapped_data_train, labels_train)\n",
    "    train_model(model, train_loader, val_loader, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdf0a60",
   "metadata": {},
   "source": [
    "# Train resnet with fc=nn.Linear() -> Bad results\n",
    "Takes around 36s/epoch on my machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ab35dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, \"train\"), transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, \"valid\"), transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "\n",
    "model = resnet18(pretrained=True)\n",
    "\n",
    "model.fc = nn.Linear(512, 4)\n",
    "<!-- # model.fc = SimpleNN(X_train, Y_train)\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for inputs, labels in train_loader:\n",
    "#         print(\"starting\")\n",
    "#         outputs = model(inputs)\n",
    "#         mapped_data = outputs\n",
    "\n",
    "# model = SimpleNN(mapped_data, labels) -->\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model.criterion = nn.CrossEntropyLoss()\n",
    "model.optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "model.scheduler = optim.lr_scheduler.StepLR(model.optimizer, step_size=10, gamma=0.9)\n",
    "\n",
    "train_model(model, train_loader, val_loader, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb17232",
   "metadata": {},
   "source": [
    "# TODO: Train resnet both ways but using batches instead of preprocessing results in one batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2998e197",
   "metadata": {},
   "source": [
    "# Data augmentation su determinate classi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e3ac03c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Trasformazione base (no augmentation)\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m base_transform \u001b[38;5;241m=\u001b[39m \u001b[43mtransforms\u001b[49m\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     27\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),\n\u001b[1;32m     28\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     29\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean, std)\n\u001b[1;32m     30\u001b[0m ])\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Augmentation solo per le classi minoritarie\u001b[39;00m\n\u001b[1;32m     33\u001b[0m augmented_transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     34\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),\n\u001b[1;32m     35\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mRandomHorizontalFlip(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean, std)\n\u001b[1;32m     41\u001b[0m ])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "#modifica ImageFolder per poter applicare DA diverse a seconda delle classi\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "class BalancedAugmentDataset(ImageFolder):\n",
    "    def __init__(self, root, transform_common=None, transform_augmented=None, classes_to_augment=None):\n",
    "        super().__init__(root, transform=None)\n",
    "        self.transform_common = transform_common\n",
    "        self.transform_augmented = transform_augmented\n",
    "        self.classes_to_augment = classes_to_augment or []\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, label = self.samples[index]\n",
    "        image = self.loader(path)\n",
    "\n",
    "        # Applica augmentation solo se la classe è tra quelle specificate\n",
    "        if self.classes[label] in self.classes_to_augment:\n",
    "            if self.transform_augmented:\n",
    "                image = self.transform_augmented(image)\n",
    "        else:\n",
    "            if self.transform_common:\n",
    "                image = self.transform_common(image)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "# Trasformazione base (no augmentation)\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Augmentation solo per le classi minoritarie\n",
    "augmented_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# provo ad aumentare le classi 1 e 3\n",
    "classes_to_augment = [\"Adgelcarcinoma\", \"Noncancer\"]\n",
    "\n",
    "# carica il training set con le nuove classi\n",
    "train_dataset = BalancedAugmentDataset(\n",
    "    root=os.path.join(dataset_path, \"train\"),\n",
    "    transform_common=base_transform,\n",
    "    transform_augmented=augmented_transform,\n",
    "    classes_to_augment=classes_to_augment\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240959ca",
   "metadata": {},
   "source": [
    "Ecco i risultati:\n",
    "Accuracy : 0.6667\n",
    "F1 Score : 0.6739\n",
    "Precision: 0.7011\n",
    "Recall   : 0.6647\n",
    "\n",
    "Detailed per-class metrics:\n",
    "                     precision    recall  f1-score   support\n",
    "\n",
    "     Adenocarcinoma       0.58      0.76      0.66       120\n",
    "     Adgelcarcinoma       0.58      0.41      0.48        51\n",
    "Squamosgelcarcinoma       1.00      1.00      1.00        54\n",
    "          Noncancer       0.64      0.49      0.55        90\n",
    "\n",
    "           accuracy                           0.67       315\n",
    "          macro avg       0.70      0.66      0.67       315\n",
    "       weighted avg       0.67      0.67      0.66       315\n",
    "\n",
    "Sostanzialmente un incremento dell'accuracy del 6% (adesso a 67%) e dell'F1 score di 10 punti (adesso a 0.67)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de39817c",
   "metadata": {},
   "source": [
    "# Loss pesata sulle classi piccole -> Più utile nella classificazione binaria dove le classi sono più sbilanciate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c92422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estrai le etichette (interi) dal dataset di training\n",
    "labels = [label for _, label in train_dataset.samples]\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np # numpy is also needed for np.unique\n",
    "\n",
    "# Calcola i pesi bilanciati per ciascuna classe\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "# Converti in tensor e porta su device (CPU o GPU)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132d835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# passo i pesi come parametro in maniera tale che durante il training gli errori sulle classi meno frequenti abbiano un peso maggiore e quindi il modello sarà incentivato a non \"ignorare\" quelle classi.\n",
    "\n",
    "# criterion = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "\n",
    "# ----------- EARLY STOPPING ------------\n",
    "# # Early stopping logic\n",
    "# if val_loss < best_val_loss:\n",
    "#     best_val_loss = val_loss\n",
    "#     epochs_without_improvement = 0\n",
    "#     best_model_state = model.state_dict()  # salva il miglior modello\n",
    "# else:\n",
    "#     epochs_without_improvement += 1\n",
    "#     print(f\"  [EarlyStopping] No improvement for {epochs_without_improvement} epoch(s)\")\n",
    "\n",
    "# if epochs_without_improvement >= patience:\n",
    "#     print(\"Early stopping triggered. Restoring best model weights.\")\n",
    "#     model.load_state_dict(best_model_state)\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
