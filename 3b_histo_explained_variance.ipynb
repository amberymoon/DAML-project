{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beef9885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import collections\n",
    "from sklearn.decomposition import PCA\n",
    "from data_augmentation import build_balanced_augmented_tensor_dataset  # calcoliamo l'exlained variance sui dati augmentati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cab67c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURAZIONE\n",
    "data_dir = 'data_histo'  # cambia percorso se necessario\n",
    "image_size = 224  # più piccolo per velocizzare il test\n",
    "batch_size = 11000 # batch size provati : 32, 800, 2048\n",
    "train_ratio, val_ratio = 0.7, 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c822be04",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Caricamento dataset bilanciato e normalizzato con z-score\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m imgs, labels, class_names \u001b[38;5;241m=\u001b[39m build_balanced_augmented_tensor_dataset(\n\u001b[0;32m      3\u001b[0m     image_folder_path\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[0;32m      4\u001b[0m     image_size\u001b[38;5;241m=\u001b[39mimage_size,\n\u001b[0;32m      5\u001b[0m     grayscale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      6\u001b[0m     normalize_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzscore\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# normalizzazione fondamentale per PCA\u001b[39;00m\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Divisione del dataset bilanciato in train/val/test\u001b[39;00m\n\u001b[0;32m     10\u001b[0m total_size \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\noemi\\Documents\\GitHub\\DAML-project\\data_augmentation.py:42\u001b[0m, in \u001b[0;36mbuild_balanced_augmented_tensor_dataset\u001b[1;34m(image_folder_path, image_size, grayscale, normalize_mode)\u001b[0m\n\u001b[0;32m     35\u001b[0m base_transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mResize((image_size, image_size))\n\u001b[0;32m     36\u001b[0m to_tensor \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mToTensor()\n\u001b[0;32m     38\u001b[0m augment_transforms \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     39\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mRandomHorizontalFlip(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m),\n\u001b[0;32m     40\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mRandomVerticalFlip(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m),\n\u001b[0;32m     41\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mRandomRotation(\u001b[38;5;241m15\u001b[39m),\n\u001b[1;32m---> 42\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mColorJitter(brightness\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, contrast\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m),\n\u001b[0;32m     43\u001b[0m ]\n\u001b[0;32m     45\u001b[0m raw_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(image_folder_path, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     46\u001b[0m class_to_paths \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\noemi\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:995\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    993\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m--> 995\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m    997\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\noemi\\anaconda3\\Lib\\site-packages\\PIL\\ImageFile.py:293\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    292\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 293\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(b)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Caricamento dataset bilanciato e normalizzato con z-score\n",
    "imgs, labels, class_names = build_balanced_augmented_tensor_dataset(\n",
    "    image_folder_path=data_dir,\n",
    "    image_size=image_size,\n",
    "    grayscale=False,\n",
    "    normalize_mode=\"zscore\"  # normalizzazione fondamentale per PCA\n",
    ")\n",
    "\n",
    "# Divisione del dataset bilanciato in train/val/test\n",
    "total_size = imgs.shape[0]\n",
    "train_size = int(train_ratio * total_size)\n",
    "val_size = int(val_ratio * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "# Mescolamento coerente\n",
    "indices = torch.randperm(total_size)\n",
    "imgs = imgs[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "train_imgs = imgs[:train_size]\n",
    "val_imgs = imgs[train_size:train_size+val_size]\n",
    "test_imgs = imgs[train_size+val_size:]\n",
    "\n",
    "# Flatten immagini per PCA\n",
    "train_data = train_imgs.view(train_imgs.size(0), -1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e3be67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TRASFORMAZIONE IMMAGINI\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((image_size, image_size)),\n",
    "#     transforms.ToTensor(),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb6908a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10500\n"
     ]
    }
   ],
   "source": [
    "# full_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "# total_size = len(full_dataset)\n",
    "# train_size = int(train_ratio * total_size)\n",
    "# val_size = int(val_ratio * total_size)\n",
    "# test_size = total_size - train_size - val_size\n",
    "# print(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d826da49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set, val_set, test_set = random_split(full_dataset, [train_size, val_size, test_size],\n",
    "#                                             generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=False)\n",
    "# val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "# test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False) \n",
    "# # droplast = True per scartare il batch non completo di 32 immagini, in questo calso è false, quindi non viene scartato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05af46ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caricamento immagini: 100%|██████████| 1/1 [02:39<00:00, 159.80s/it]\n"
     ]
    }
   ],
   "source": [
    "# # ESTRAZIONE E FLATTENING IMMAGINI\n",
    "# def extract_data(loader):\n",
    "#     data = []\n",
    "#     for images, _ in tqdm(loader, desc=\"Caricamento immagini\"): # prende il batch del data loader\n",
    "#         flat = images.view(images.size(0), -1)  # flatten [B, C, H, W] → [B, C*H*W]\n",
    "#         data.append(flat) #collezione dei dati del batch appiattiti\n",
    "#     data = torch.cat(data, dim=0) # concatena tutti i batch delle immagini\n",
    "#     return data.numpy()\n",
    "\n",
    "# train_data = extract_data(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd4c0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calcolo normalizzazione globale...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calcolo min/max globali: 100%|██████████| 1/1 [01:55<00:00, 115.47s/it]\n"
     ]
    }
   ],
   "source": [
    "# def compute_min_max(loader):\n",
    "#     global_min, global_max = float('inf'), float('-inf')\n",
    "#     for images, _ in tqdm(loader, desc=\"Calcolo min/max globali\"):\n",
    "#         flat = images.view(images.size(0), -1).numpy().astype(np.float32)\n",
    "#         batch_min = flat.min()\n",
    "#         batch_max = flat.max()\n",
    "#         global_min = min(global_min, batch_min)\n",
    "#         global_max = max(global_max, batch_max)\n",
    "#     return global_min, global_max\n",
    "\n",
    "# print(\"\\nCalcolo normalizzazione globale...\")\n",
    "# global_min, global_max = compute_min_max(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4d96f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\nInizio Incremental PCA...\")\n",
    "# ipca = IncrementalPCA(n_components=None, batch_size=batch_size)\n",
    "# # versione di PCA che non carica tutto il dataset in RAM, ma lavora a batch\n",
    "\n",
    "# for images, _ in tqdm(train_loader, desc=\"Fit IPCA\"):\n",
    "#     flat = images.view(images.size(0), -1).numpy().astype(np.float32)\n",
    "#     flat = (flat - global_min) / (global_max - global_min + 1e-8)  # normalizza batch per evitare divisione per 0\n",
    "#     ipca.partial_fit(flat)\n",
    "# # fitting parziale del modello PCA con il batch corrente, ripete questa operazione su tutti i batch, \n",
    "# # internamente accumula media, covarianza, varianza ecc. per stimare le componenti principali finali.\n",
    "\n",
    "# # EXPLAINED VARIANCE\n",
    "# explained_variance = ipca.explained_variance_ratio_\n",
    "# cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "\n",
    "# PCA\n",
    "pca = PCA()\n",
    "pca.fit(train_data)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43644a24",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cumulative_variance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# GRAFICO VARIANZA CUMULATIVA\u001b[39;00m\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(cumulative_variance)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), cumulative_variance, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39maxhline(y\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m95\u001b[39m\u001b[38;5;132;01m% s\u001b[39;00m\u001b[38;5;124moglia\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39maxhline(y\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.85\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m85\u001b[39m\u001b[38;5;132;01m% s\u001b[39;00m\u001b[38;5;124moglia\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cumulative_variance' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GRAFICO VARIANZA CUMULATIVA\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(cumulative_variance)+1), cumulative_variance, marker='.')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n",
    "plt.axhline(y=0.85, color='b', linestyle='--', label='85% threshold')\n",
    "plt.axhline(y=0.75, color='g', linestyle='--', label='75% threshold')\n",
    "plt.axhline(y=0.65, color='c', linestyle='--', label='65% threshold')\n",
    "plt.axhline(y=0.50, color='m', linestyle='--', label='50% threshold')\n",
    "plt.xlabel('Number of principal components')\n",
    "plt.ylabel('Explained variance')\n",
    "plt.title('PCA, explained variance for histological images')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b87e50",
   "metadata": {},
   "source": [
    "è molto strano che le componenti totali siano poco più di 30, quando abbiamo tante immagini (14.996) a colori e 224 x 224 \n",
    "eseguiamo una serie di test per vedere se tutto funziona correttmente (cella 12) e vedere come comportarci successivamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20ff2e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Componenti che spiegano almeno il 95% della varianza: 2252\n",
      "Componenti che spiegano almeno il 85% della varianza: 725\n",
      "Componenti che spiegano almeno il 75% della varianza: 336\n",
      "Componenti che spiegano almeno il 65% della varianza: 165\n",
      "Componenti che spiegano almeno il 50% della varianza: 46\n"
     ]
    }
   ],
   "source": [
    "# STAMPA NUM COMPONENTI PER SOGLIA\n",
    "soglie = [0.95, 0.85, 0.75, 0.65, 0.50]\n",
    "for soglia in soglie:\n",
    "    num_components = np.argmax(cumulative_variance >= soglia) + 1\n",
    "    print(f\"Componenti che spiegano almeno il {int(soglia*100)}% della varianza: {num_components}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137e0751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_counts = collections.Counter(train_labels.tolist())\n",
    "# plt.bar(pretty_labels, [label_counts.get(i, 0) for i in range(len(pretty_labels))])\n",
    "# plt.title(\"Distribuzione delle classi - Train\")\n",
    "# plt.xticks(rotation=15)\n",
    "# plt.ylabel(\"Numero campioni\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f69e812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Numero immagini (n_samples): {train_data.shape[0]}\")\n",
    "# print(f\"Numero feature per immagine (n_features): {train_data.shape[1]}\")\n",
    "# sample = next(iter(train_loader))[0]\n",
    "# print(\"Shape singola immagine:\", sample[0].shape)  # dovresti avere (3, 224, 224)\n",
    "# print(f\"Componenti totali calcolate: {len(ipca.explained_variance_ratio_)}\")\n",
    "# print(train_data.shape)\n",
    "# print(ipca.n_components_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85c6eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANTE: con IncrementalPCA, il numero massimo di componenti principali calcolabili\n",
    "# è limitato a n_components <= min(batch_size, n_features).\n",
    "# \n",
    "# Nel nostro caso:\n",
    "# - batch_size = 32\n",
    "# - n_features = 150528 (immagini RGB 224x224 → 3*224*224)\n",
    "# \n",
    "# Quindi il numero massimo di componenti principali che possono essere apprese per batch è 32.\n",
    "# \n",
    "# Di conseguenza, anche se specifichiamo n_components = None (per calcolare tutte le possibili componenti),\n",
    "# IncrementalPCA si ferma a 32 componenti, perché il batch ha solo 32 immagini.\n",
    "#\n",
    "# Per ottenere più componenti, è necessario aumentare il batch_size (es. 128 o 256),\n",
    "# oppure usare PCA standard se la memoria lo consente.\n",
    "\n",
    "# non si riesce a usare PCA -> torniamo a IPCA\n",
    "# utilizziamo il batch maggiore che la RAM permette, calcoliamo la % di PC per ogni threshold\n",
    "# per trovare le PC per ogni threshold su tutto il dataset faremo la media sulle precedenti\n",
    "\n",
    "# passando da batch di 1024 a 2028 le PC sono diminuite, ma non di tanto \n",
    "# siamo passati da 739 a 728, eppure i dati su cui fare PCA sono raddoppiati, quindi\n",
    "# le percentuali sono notevolmente diminuite, non ha senso guardare queste\n",
    "\n",
    "# noto per 95% mi ese una sola PC, ma ciò non ha senso poichè per l'85% ne abbiamo 728\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
